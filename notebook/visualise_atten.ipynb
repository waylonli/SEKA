{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7107522224bc31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/AccelWorld/opt/anaconda3/envs/seka/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/Volumes/AccelWorld/opt/anaconda3/envs/seka/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2639f3d28caace8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.30s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch, gc, seaborn as sns, matplotlib.pyplot as plt\n",
    "import sys  \n",
    "sys.path.insert(1, '../')\n",
    "from src.model import SEKALLM\n",
    "from src.utils import encode_with_markers\n",
    "\n",
    "# ---------- 1. prompt -------------------------------------------------\n",
    "tok      = AutoTokenizer.from_pretrained('../pretrained/Qwen3-4B-Base')\n",
    "ks = SEKALLM(\"../pretrained/Qwen3-4B-Base\", output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7efa35cf7bdd139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Steering hooks attached on layers [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]\n",
      "✓ collected baseline and steered attentions\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Previously Patrick Roy professionally plays the sport hockey. Currently Patrick Roy **professionally plays the sport basketball**. Patrick Roy is a professional\"\n",
    ")\n",
    "ids, msk, _ = encode_with_markers(prompt, tok)        # ids:(1,seq)  msk:(seq,)\n",
    "ids      = ids.to(\"mps\");  device = ids.device\n",
    "\n",
    "ks.remove_projection()\n",
    "\n",
    "with torch.no_grad():\n",
    "    base_out  = ks.model(ids, output_attentions=True, use_cache=False)\n",
    "base_attn = [l.detach().cpu() for l in base_out.attentions]\n",
    "\n",
    "# ---------- 3. inject φ‑space K‑projection ---------------------------\n",
    "ks.attach_projection(\n",
    "    pos_pt=\"../projections/synthetic_new/Qwen3-4B-Base_pos_proj.pt\",\n",
    "    neg_pt=\"../projections/synthetic_new/Qwen3-4B-Base_neg_proj.pt\",\n",
    "    layers=\"all\",\n",
    "    steer_mask_tensor=msk,\n",
    "    amplify_pos=1.5,                # tune as needed\n",
    "    amplify_neg=0.3,                # tune as needed\n",
    "    # feature_function=\"squared-exponential\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    steer_out = ks.model(ids, output_attentions=True, use_cache=False)\n",
    "steer_attn = [l.detach().cpu() for l in steer_out.attentions]\n",
    "\n",
    "gc.collect()\n",
    "print(\"✓ collected baseline and steered attentions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df8caea81759dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:06<00:00,  3.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# ---------- 4. visual helper -----------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from tqdm import tqdm\n",
    "\n",
    "def show_pair(b_attn, s_attn, layer: int, head: int | None = None, vmax: float | None = None):\n",
    "    B = b_attn[layer][0]\n",
    "    S = s_attn[layer][0]\n",
    "    if head is None:\n",
    "        B = B.mean(0)\n",
    "        S = S.mean(0)\n",
    "        ttl = f\"L{layer+1} | mean of all heads\"\n",
    "    else:\n",
    "        B = B[head]\n",
    "        S = S[head]\n",
    "        ttl = f\"L{layer+1}, H{head+1}\"\n",
    "    vmax = float(max(B.max(), S.max())) if vmax is None else vmax\n",
    "    tokens = tok.convert_ids_to_tokens(ids[0].tolist())\n",
    "\n",
    "    cmap = LinearSegmentedColormap.from_list(\"custom\", [\"#fffbe0\", \"#006400\"])\n",
    "    mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "    fig_width = max(12, 0.32 * len(tokens))\n",
    "    fig = plt.figure(figsize=(fig_width, 5))\n",
    "    gs = GridSpec(1, 2, width_ratios=[1,1], wspace=0.04, left=0.06, right=0.88, bottom=0.15, top=0.87)\n",
    "\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "    for ax, mat, title in zip([ax1, ax2], [B, S], [\"Original\", \"SEKA\"]):\n",
    "        sns.heatmap(\n",
    "            mat.numpy(), ax=ax, cmap=cmap,\n",
    "            vmin=0, vmax=vmax, cbar=(ax is ax2), square=True\n",
    "        )\n",
    "        ax.set_title(f\"{title} - {ttl}\", fontsize=16, fontname=\"Times New Roman\")\n",
    "        ax.set_xticks(range(len(tokens)))\n",
    "        ax.set_xticklabels(tokens, rotation=90, fontsize=9, fontname=\"Times New Roman\")\n",
    "        ax.set_yticks(range(len(tokens)))\n",
    "        ax.set_yticklabels(tokens, rotation=0, fontsize=9, fontname=\"Times New Roman\")\n",
    "    \n",
    "    # fig.savefig(f\"attention_visualisation/Qwen3-4B-Base-L{layer+1}.pdf\", bbox_inches=\"tight\")\n",
    "    # plt.close()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ---------- 5. example ------------------------------------------------\n",
    "\n",
    "# for l in tqdm(range(15,36)):\n",
    "#     show_pair(base_attn, steer_attn, layer=l, head=None)   # avg heads\n",
    "show_pair(base_attn, steer_attn, layer=25, head=8)    # one head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7387133e6a7311d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
